# Masakhane Meeting Notes: 14/05/2020

● Time: 19:00
● Number of Attendees: 19
● Notes taken by Lerato Louis

# Introduction/ Progress Update

● Jade welcomes all attendee’s and runs a quick scan to see if  there are new members, given the positive spike in attendance by new comers, last week. This spike seems to have been a consequence of the ICRL 2020 conference (where we had 9 papers published), as well as a televised BBC mention the foundation received.
● Jade touches on her intention to recreate the meeting, possibly changing it’s link, so she can have a co-host/(s), who will assist with monitoring chats and general meeting related admin.
● Jade then proposes the intended structure of  today’s meeting, listing the following as areas of importance/ area’s that need to be covered: 
      ● Brief  intro as to where the group’s currently at
      ● Initiatives - data sets, who’s building data sets, moving though the other initiatives
      ● Usage of  tools 
      ● Open floor
● Those who may be new and still be figuring out where they’d want to to fit in (especially those with a background in software) are encouraged to visit Masakhane’s Github repo’s to see what they can get started on. She touches on Masakhane’s first initiative (training a machine translation model for your language) as a good place to start, as well as the language benchmarks (about 40 languages) that Masakhane currently has.
● Jade touches on how the restructuring of  the community, to accommodate all NLP tasks has made the community more inclusive, allowing more people to join.
● She also mentions one of the milestones Masakhane is currently working towards, the EMNLP conference and the importance of honing in on what it is the community is about, the problems we’re trying to solve and on the community’s value proposition and what sets the community apart. 


# Data/ Data set Creation

● Bonaventure - Managed to collect and label the data he mentioned last week. He now has about 500 sentences. His next step is to try and rebuild, working through the EMNLP task, determining the low resourced data’s bleuscore. He also intends to collect all the Dendi related booklets his data source wrote and include their contents in his data set. This is to increase the size of his data set, and allow him to continue training. Kelechi joined the conversation and encouraged Bona to take a pre-trained model and use that to fine tune his current data set.
● Vilemina (and Musi) - Have managed to scrape and manually clean some data from the music subset of the JW website. Their language of choice being Damara. The number of sentences she’s scraped is between 500 and 600, where each song has about 3 to 4 verses. The current data they have, which is clean (for the parallel caucus) is 63 sentences. The problem is, that on 20 songs they have 63 clean sentences, so she’s not sure that that’ll be enough to do much with. Another part of the problem, which is delaying the scraping, is the current mismatch between the English and Damara links (on the website). They’ve had help reviewing the data, from native speakers.
● Julia - Made reference to the paper that was under review in this weeks reading group (https://arxiv.org/abs/2001.04451). She noted the importance of the decoder language being common/ shared, particularly in instances where the translation is from other languages (target languages) into English (source language). Since this makes the translation easier.  She notes that its possible to do data augmentation use Masakhane’s existing models to back translate target data into the desired language. She also highlights the importance of having the correct pre-processing, when using preexisting models.
● Bonaventure - References a paper he’s recently read (as an offshoot of  what was discussed in the Reading Group), which details how a trained translation model can be act as a parent to the target language, making it possible swap the source and target language, while significantly improving  transfer. This eliminates the need to have a shared decoder language.
● David - Has updates concerning name density recognition. His call to action is to get everyone that speaks the listed languages, to help with annotation and with technically crawling the data source. The chosen data source or focus being news websites. A key feature of the process is, to analyze the quality, biases and accuracy of the data, and technically report back on any and all, issues found. Timi then asked where the data would be stored. To which Jade suggested, Github. Timi then proposed creating a private yet shared repo, for internal data storage, possibly with two separate folders for vetted and unvetted data. Which raised the issue of discoverability, with Jade. To which David suggested only publishing verified data. Timi, Jade and David weighed the pro’s and con’s of storing the data on a Google Drive and eventually agreed to storing the data on Github (possibly through smaller repo’s or Git large file storage). David then pointed out the need to create a data sheets for those data sets. Which created a segue into updates on Emily Bender’s data set workshop, which David and Oravei expand upon.
● Oravei (who was paired with David at the workshop) - The workshop focused on how one creates one’s data. She was working on the Pidgin data set that they used for unsupervised NMC, which was scraped from BBC New’s. She encouraged everyone to watch Emily’s presentation which David has shared.
● Bonaventure - Raised an SSH break related query, to which Jade suggested the use of  Screen, to allow for the detached usage of SSH, preventing the undesired loss of processes, should there be connection breaks.
● ? - Raised a data alignment issue. Which was deferred to Moses. Given Moses absence, Julia (who had previously shared Moses’ tools with ?) stepped in, asking a few questions, to better understand the nature of  the issue. She then recommends, as a starting point, checking the extraction from the XML and debugging it. 

# Initiative and Paper Updates

● Julia and  Elan (on the COVID survey translations) - No updates yet 

# Other notes

● Timi - here are some of the potential institutions people can partner with:
Living Tongues Institute for Endangered Languages
      ● http://www.endangeredlanguagefund.org/
      ● http://7000.org
      ● http://www.iie.org/Give-to-IIE#.WD2fceYrKM8
● Bona - Encourages everyone to join the weekly Reading Group
● Jade - Highlights a funding opportunity that will soon come up. Updates coming soon.
● Jade - On the subject of scraping from news websites, Masakhane is potentially looking at getting a copyright lawyer who will be able to advice us on copyright issues. Particularly on the ethical aspect of  republishing.

Then Lerato stopped taking notes, but if anyone can help expand these, that would be useful.
